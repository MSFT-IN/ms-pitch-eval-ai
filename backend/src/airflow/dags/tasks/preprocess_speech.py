
import os
import numpy as np
import librosa
import soundfile as sf

def preprocess_speech(
    input_wav,
    sr=16000,
    top_db=20,
    max_duration_sec=90,
    silence_threshold_sec=1,
    silence_db_threshold=-40,
    normalize=True
):
    """
    ê³ í’ˆì§ˆ ìŒì„± ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ (ìŒì§ˆ ë³´ì¡´ìš©):
    1. ìŠ¤í…Œë ˆì˜¤ ìœ ì§€ & ê³ ê¸‰ ë¦¬ìƒ˜í”Œë§ (kaiser_best)
    2. Normalize (Peak ê¸°ë°˜, clipping ë°©ì§€)
    3. silence_db_threshold ì´í•˜ ë§ˆìŠ¤í‚¹ (ë¬´ìŒ ì²˜ë¦¬)
    4. ì•ë’¤ ë¬´ìŒ íŠ¸ë¦¬ë°
    5. 1ì´ˆ ì´ìƒ ì¹¨ë¬µ ì œê±° (ë„ˆë¬´ ê¸´ ì¹¨ë¬µì€ ì œê±°, ì§§ì€ ì¹¨ë¬µì€ ë¶™ì„)
    6. ìµœëŒ€ 90ì´ˆë¡œ ìë¥´ê¸°
    7. ì•ˆì „í•œ íŒŒì¼ëª…ìœ¼ë¡œ *_trimmed.wav ë¡œ ì €ì¥ (PCM_16 í˜•ì‹)
    """
    print(" ê³ ìŒì§ˆ ì˜¤ë””ì˜¤ ë¡œë”© ì¤‘...")
    # mono=Falseë¡œ ìŠ¤í…Œë ˆì˜¤ ìœ ì§€
    y, _ = librosa.load(input_wav, sr=sr, mono=False, res_type="kaiser_best")

    if normalize:
        print("ğŸšï¸ Peak ê¸°ë°˜ ì •ê·œí™” ì¤‘...")
        peak = np.max(np.abs(y))
        if peak > 0:
            y = y / peak * 0.9

    print(" ì•ë’¤ ë¬´ìŒ íŠ¸ë¦¬ë° ì¤‘...")
    y_trimmed, _ = librosa.effects.trim(y, top_db=top_db)

    print(" ê¸´ ì¹¨ë¬µ êµ¬ê°„ ì œê±° ì¤‘...")
    intervals = librosa.effects.split(y_trimmed, top_db=top_db)
    max_gap = int(silence_threshold_sec * sr)

    voiced_segments = []
    total_len = 0
    max_total_len = int(max_duration_sec * sr)

    for i, (start, end) in enumerate(intervals):
        segment = y_trimmed[..., start:end]  # works for mono (1D) or stereo (2D)
        if i < len(intervals) - 1:
            next_start = intervals[i + 1][0]
            gap = next_start - end
            if gap <= max_gap:
                # Append gap segment if short
                segment = np.concatenate([segment, y_trimmed[..., end:next_start]], axis=-1)
        voiced_segments.append(segment)
        # Accumulate total length along the time axis.
        if segment.ndim == 1:
            total_len += segment.shape[0]
        else:
            total_len += segment.shape[-1]
        if total_len >= max_total_len:
            break

    if not voiced_segments:
        raise ValueError(" ìœ íš¨í•œ ë°œí™” êµ¬ê°„ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")

    # Concatenate segments along the time axis
    if y_trimmed.ndim == 1:
        final_audio = np.concatenate(voiced_segments)[:max_total_len]
    else:
        final_audio = np.concatenate(voiced_segments, axis=-1)[:, :max_total_len]

    # ì•ˆì „í•œ íŒŒì¼ëª… ë§Œë“¤ê¸° (íŠ¹ìˆ˜ë¬¸ì ì œê±° ê¶Œì¥)
    base, _ = os.path.splitext(os.path.basename(input_wav))
    output_wav = f"{base}_trimmed.wav"

    # SoundFile expects shape (samples, channels)
    if final_audio.ndim == 2 and final_audio.shape[0] == 2:
        # current shape is (2, T) -> transpose to (T, 2)
        final_audio = final_audio.T

    sf.write(output_wav, final_audio, sr, subtype="PCM_16")

    # For reporting, if stereo, use number of samples / sr as duration from axis 0
    duration = final_audio.shape[0] / sr if final_audio.ndim == 2 else len(final_audio)/sr
    print(f"ê³ í’ˆì§ˆ ì „ì²˜ë¦¬ ì™„ë£Œ: {output_wav} (ê¸¸ì´: {duration:.2f}ì´ˆ)")
    return output_wav